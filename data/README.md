# MultiMUC Data

All of the MultiMUC data can be found in the `multimuc_v1.0.zip` archive in this directory, which contains of the following:

- `en/`: The gold English MUC-4 data in JSONlines formatted, as released in [this repo](https://github.com/xinyadu/gtt).
- `uncorrected/`: The *uncorrected* automatically translated and projected MUC-4 data for each of the MultiMUC target languages: Arabic (`ar`), Chinese (`zh`), Farsi (`fa`), Korean (`ko`), and Russian (`ru`). These files are formatted identically to the English MUC-4 data, in JSONlines format, where each example contains the following information:
  - `docid`: The unique identifier for the example.
  - `doctext`: The full text of the document associated with the example.
  - `templates`: A list of the automatically projected template annotations. Each template contains a string-valued `incident_type` field, denoting the type of the template, as well as five entity-valued slots: `PerpInd` (individual perpetrators), `PerpOrg` (organizational perpetrators), `Victim` (human victims of the incident), `Target` (physical targets of th_e incident), and `Weapon` (weapons used in the incident). The entity fillers of these slots are specified as lists of mentions, where each mention consists of (1) the mention string, and (2) the character offset of the first occurrence of this string in `doctext`.
- `corrected/`: The *corrected* data for each of the MultiMUC target languages. As we note in the paper, the nature of these corrections is different for the train split (on the one hand) and for the dev and test splits (on the other). For the `train.jsonl` files, the automatic translations are left unchanged and only the boundaries of slot-filling entity mentions in those translations have been manually corrected in each target language. For the `dev.jsonl` and `test.jsonl` files, in addition to corrections to the boundaries of slot-filling entity mentions, the translations of the sentences containing these mentions have also been manually corrected. These files are formatted in the same way as the files in `uncorrected/`, except that each example additionally contains a `sentences` field that lists the boundaries (given as character offsets) of the sentences in `doctext`.

Two additional notes:

- The original MUC-4 data features some entity coreference information in the sense that it often lists several coreferent strings that appear in the document for a given entity. However, these strings are not accompanied by textual offsets and furthermore are limited to names and nominals (excluding pronouns). As such, the entity coreference information provided by the annotations is incomplete. We follow [Du et al. (2021)](https://aclanthology.org/2021.naacl-main.70/) in focusing on the *first* occurrence of each gold annotated string in the document. Therefore, the corrections in the `corrected/` data are limited to these occurrences (for span boundary corrections) and to the sentences that contain them (for translation correction).
- The predictions included in the `predictions` directory and all results reported in the paper are based on the *corrected* test files. Please report results on these files if attempting to make comparisons with our results.
